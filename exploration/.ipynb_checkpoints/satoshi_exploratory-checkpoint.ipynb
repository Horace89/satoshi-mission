{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division  # no need to worry about interger division\n",
    "\n",
    "from pymongo import MongoClient\n",
    "import psycopg2 as pg2\n",
    "import json\n",
    "\n",
    "from pandas.io.sql import read_sql\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from itertools import chain, izip\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set(font='Avenir', style='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../../auth/postgres/postgres.json', 'r') as f:\n",
    "    PGCONN = json.load(f)\n",
    "\n",
    "with open('../.dbname', 'r') as f:\n",
    "    PGCONN['dbname'] = json.load(f)['dbname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conn = pg2.connect(**PGCONN)\n",
    "\n",
    "q = '''\n",
    "    SELECT * FROM basic_stats2\n",
    "    WHERE author = 'satoshi';\n",
    "    '''\n",
    "\n",
    "df = read_sql(q, conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.groupby('type').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sum = df.groupby('type').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentences:\n",
    "\n",
    "### Observations:\n",
    "* hard to extract paragraph from e-mail and forum posts\n",
    "    * sentence/paragraph not comparable between paper/email vs. forum (mostly answering people's question)\n",
    "* word count: \n",
    "    * paper ~ 3000; \n",
    "    * email ~ 5000; \n",
    "    * forum ~ 50000\n",
    "* space after period: \n",
    "    * paper: ~one space\n",
    "    * email: ~two spaces\n",
    "    * forum: ~one space\n",
    "    * periods_twospaces/periods_onespace -- scale goes from 0 to inf -- not a good feature\n",
    "* sentences:\n",
    "    * upper + lower ~= 88% of punkt -- use punkt\n",
    "\n",
    "    \n",
    "### Preliminary set of features author attribution (satoshi vs. satoshi):\n",
    "* periods_onespace/sentences_punkt\n",
    "* periods_twospaces/sentences_punkt\n",
    "* words/sentence_punkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sum[['words',\n",
    "        'periods_nospace',\n",
    "        'periods_onespace',\n",
    "        'periods_twospaces']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sum[['sentences_punkt',\n",
    "        'sentences_upper',\n",
    "        'sentences_lower']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(df_sum['sentences_upper'] + df_sum['sentences_lower']) / df_sum['sentences_punkt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sum['words/sentences_punkt']       = df_sum['words'] / df_sum['sentences_punkt']\n",
    "df_sum['words/sentences_upper_lower'] = df_sum['words'] / (df_sum['sentences_upper'] + df_sum['sentences_lower'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sum[['words/sentences_punkt',\n",
    "        'words/sentences_upper_lower']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words:\n",
    "\n",
    "### Observations:\n",
    "* all types of writing: words_US >= words_GB -- how much of the difference is because of the dictionaries used?\n",
    "\n",
    "### Preliminary set of features for author attribution (satoshi vs. satoshi):\n",
    "* words_nostop/words\n",
    "* words_primaryvb/words\n",
    "* words_to/words\n",
    "* words_verb/words\n",
    "* words_noun/words\n",
    "* words_desc/words\n",
    "* words_det/words\n",
    "* words_conj/words\n",
    "* words_gb/words_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sum['words_nostop/words']    = df_sum['words_nostop']    / df_sum['words']\n",
    "df_sum['words_primaryvb/words'] = df_sum['words_primaryvb'] / df_sum['words']\n",
    "df_sum['words_to/words']        = df_sum['words_to']        / df_sum['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sum[['words_nostop/words',\n",
    "        'words_primaryvb/words', \n",
    "        'words_to/words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_types = {'verb': ['vb', 'vbd', 'vbn', 'vbp', 'vbz'],\n",
    "              'noun': ['nn', 'nns', 'nnp', 'nnps', 'prp', 'pp$'],                          # noun, pronoun\n",
    "              'desc': ['jj', 'jjr', 'jjs', 'rb', 'rbr', 'rbs', 'wdt', 'wp', 'wp$', 'wrb'], # adj, adv, \"which what how, etc\"\n",
    "              'det' : ['dt', 'pdt'],                                                       # determiner\n",
    "              'conj': ['cc', 'in']}                                                        # conjugate-like\n",
    "\n",
    "cnames  = set(df.columns)\n",
    "\n",
    "col_map = dict()\n",
    "for key, lst in word_types.iteritems():\n",
    "    cname = 'words_%s' % key\n",
    "    col_map[cname] = set('words_' + c for c in lst)\n",
    "    df_sum[cname]  = df_sum[list(cnames & col_map[cname])].sum(axis=1)\n",
    "    df_sum['%s/words' % cname] = df_sum[cname] / df_sum['words']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sum[[a+b for a,b in izip(col_map.keys(), ['/words']*len(col_map))]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sum[['words_us', 'words_gb']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sum['words_us/words']    = df_sum['words_us'] / df_sum['words']\n",
    "df_sum['words_gb/words']    = df_sum['words_gb'] / df_sum['words']\n",
    "df_sum['words_gb/words_us'] = df_sum['words_gb'] / df_sum['words_us']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sum[['words_us/words', 'words_gb/words', 'words_gb/words_us']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spellings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client = MongoClient()\n",
    "db  = client['satoshi']\n",
    "tbl = db['word-lists2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dic = dict()\n",
    "\n",
    "for wt in ['paper', 'email', 'forum']:\n",
    "    \n",
    "    query_results = tbl.find( { 'author' : 'satoshi',\n",
    "                                'type'   : wt         } )\n",
    "\n",
    "    # cursor only in one direction!\n",
    "    words = [(result['misspellings'], result['us_spellings'], result['gb_spellings']) for result in query_results]\n",
    "    dic['%s_misspell' % wt], dic['%s_US' % wt], dic['%s_GB' % wt] = zip(*words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total words\n",
    "for wt in ['paper', 'email', 'forum']:\n",
    "    print wt+':', len(list(chain(*dic['%s_US' % wt]))), len(list(chain(*dic['%s_GB' % wt])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# unique words\n",
    "for wt in ['paper', 'email', 'forum']:\n",
    "    print wt+':', len(set(chain(*dic['%s_US' % wt]))), len(set(chain(*dic['%s_GB' % wt])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Word list... should have lemmatized!!\n",
    "* vbs in different tense...\n",
    "* singular/plural nouns\n",
    "* lemmatize **DOES NOT** mean change word to another word with same meaning e.g. Honda -> car\n",
    "* add lemmatizer and start all over again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "words = []\n",
    "for word in 'if you red thinking of'.split():\n",
    "    # filter -- if word can be many POS, select the POS matching the filter\n",
    "    v = lemmatizer.lemmatize(word, pos='v')  \n",
    "    if v == word:\n",
    "        v = lemmatizer.lemmatize(word, pos='n')  \n",
    "    words += v.encode('ascii'),\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
